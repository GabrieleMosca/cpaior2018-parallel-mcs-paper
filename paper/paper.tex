% vim: set spell spelllang=en tw=100 :

\documentclass{llncs}
\usepackage[pass,showframe]{geometry}

\usepackage{complexity}
\usepackage{microtype}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{cleveref}
\usepackage{tikz}

\usetikzlibrary{decorations, decorations.pathreplacing, calc, backgrounds}

\definecolor{uofgsandstone}{rgb}{0.321569, 0.278431, 0.231373}
\definecolor{uofglawn}{rgb}{0.517647, 0.741176, 0}
\definecolor{uofgcobalt}{rgb}{0, 0.615686, 0.92549}
\definecolor{uofgpumpkin}{rgb}{1.0, 0.72549, 0.282353}
\definecolor{uofgthistle}{rgb}{0.584314, 0.070588, 0.447059}

\newcommand{\citet}[1]{\citeauthor{#1} \shortcite{#1}}
\newcommand{\citep}[1]{\cite{#1}}

% cref style
\crefname{figure}{Figure}{Figures}
\Crefname{figure}{Figure}{Figures}

% http://tex.stackexchange.com/questions/22100/the-bar-and-overline-commands
\newcommand{\shortoverline}[1]{\mkern 1.5mu\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}

\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}

\title{Observations from Parallelising Three Maximum Common (Connected) Subgraph Algorithms}

\author{
    Ruth Hoffmann \inst{1}
    \and Ciaran McCreesh\thanks{This work was supported by the Engineering and Physical Sciences Research Council [grant numbers EP/K503058/1, EP/M508056/1, and EP/M508056/1]} \inst{2}
    \and Samba Ndojh Ndiaye\thanks{This work was supported by the ANR project SoLStiCe (ANR-13-BS02-0002-01)} \inst{3}
    \and Patrick Prosser \inst{2}
    \and Craig Reilly \samethanks[1] \inst{2}
    \and Christine Solnon\samethanks[2]\inst{4} \and James Trimble\samethanks[1] \inst{2}}

\institute{
    University of St Andrews, St Andrews, Scotland
    \and University of Glasgow, Glasgow, Scotland
    \and Universit\'e Lyon 1, LIRIS, UMR5205, F-69621, France
    \and INSA-Lyon, LIRIS, UMR5205, F-69621, France \\
    \email{c.mccreesh.1@research.gla.ac.uk}}

\begin{document}

\maketitle

\begin{abstract}

\end{abstract}

\section{Introduction}

\section{Sequential Algorithms}

\subsection{A Basic Constraint Model}

\subsection{Ensuring Connectedness}

\subsection{The Splitting Algorithm}

Due to the special structure of the maximum common subgraph problem, the following property holds
throughout the search process using the CP model: any two variables either have domains with no
values in common (with the possible exception of $\bot$), or have identical domains. The algorithm
McSplit exploits this property, while exploring essentially the same search tree as the CP model.
Rather than storing a domain for each vertex in $\operatorname{V}(G)$, which results in redundant
storage in cases where domains are identical, equivalence classes of vertices in
$\operatorname{V}(G)$ are stored, and a set of candidate vertices in $H$ is maintained for each
equivalence class. This enables fast propagation of the constraints and smaller memory requirements.
In addition, the data structure enables good branching heuristics to be calculated cheaply.

\subsection{The Clique Encoding}

\subsection{The $k$-less Subgraph Isomorphism Algorithm}

The algorithm as described by Hoffmann, McCreesh, and Reilly \cite{DBLP:conf/aaai/HoffmannMR17} approaches the subgraph isomorphism problem by asking the question ``if a pattern graph cannot be found in the target, how much of the pattern graph can be found?''. The $k{\downarrow}$ algorithm tries to solve the subgraph isomorphism problem first for $k=0$ (the whole pattern graph can be found in the target), should that be not satisfiable, it tries to solve the problem for $k=1$ (one vertex cannot be matched) should that also not be satisfiable, it iterates $k$ further by a small number to keep the shrinking pattern graph within a reasonable size. By unmatching some vertices  invariants are weakened but kept effective. Specifically, invariants based around paths and degrees of vertices hold.

This technique bridges the subgraph isomorphism problem with the maximal common subgraph problem. The iteration of the decision problem (iterating over $k$) leads to an algorithm with rivalling runtimes.

\section{Parallel Search}

\section{Experiments}

\begin{figure}[p]
    \includegraphics*[scale=0.7]{plots/33v-par-cumulative.pdf}
    \hfill
    \includegraphics*[scale=0.7]{plots/plain-par-cumulative.pdf}

    \vspace*{1em}
    \includegraphics*[scale=0.7]{plots/33ved-par-cumulative.pdf}
    \hfill
    \includegraphics*[scale=0.7]{plots/plain-connected-par-cumulative.pdf}

    \vspace*{1em}
    \includegraphics*[scale=0.7]{plots/33ve-connected-par-cumulative.pdf}
    \hfill
    \includegraphics*[scale=0.7]{plots/sip-par-cumulative.pdf}

    \caption{The cumulative number of instances solved in under a certain time, comparing sequential
    (solid lines) and parallel (dotted lines) algorithms, for six different families of maximum
    common (connected) subgraph problems. All experiments use 32 threads on a 16 core,
    hyper-threaded system.}
    \label{figure:mcs-par-unconnected-cumulative}
\end{figure}

Cumulative performance plots comparing each parallel algorithm to its sequential version are given
in \cref{figure:mcs-par-unconnected-cumulative}, using 32 threads on 16 core hyper-threaded Intel Xeon
E5-2640 v2 systems. The results are consistent: in each case the parallel algorithm is clearly and
substantially better than its sequential version, except when the sequential runtime is below 100
milliseconds.

\begin{figure}[p]
    \centering
    \hspace*{0.5em}\includegraphics*[scale=0.7]{plots/plain-par-heatmap-clique.pdf}\hfill
    \includegraphics*[scale=0.7]{plots/plain-par-heatmap-kdown.pdf}\hfill
    \includegraphics*[scale=0.7]{plots/plain-par-heatmap-james.pdf}\hspace*{0.5em}

    \vspace*{2em}
    \centering
    \hspace*{0.5em}\includegraphics*[scale=0.7]{plots/33ved-par-heatmap-clique.pdf}\hfill
    \includegraphics*[scale=0.7]{plots/33ved-par-heatmap-james.pdf}\hspace*{0.5em}

    \vspace*{2em}
    \centering
    \hspace*{0.5em}\includegraphics*[scale=0.7]{plots/sip-par-heatmap-kdown.pdf}\hfill
    \includegraphics*[scale=0.7]{plots/sip-par-heatmap-james.pdf}\hspace*{0.5em}

    \caption{The benefits of parallelism, for different sets of problem instances and for different
    algorithms. All experiments use 32 threads on a 16 core, hyper-threaded system; points below the
    top diagonal line indicate a speedup, and the lower diagonal line is a sixteen times speedup.}
    \label{figure:mcs-par-speedups}
\end{figure}

\Cref{figure:mcs-par-speedups} gives instance-by-instance comparisons for a representative subset of
these algorithm and problem combinations. For the clique algorithm on unlabelled graphs (top left),
the results show fairly consistent speedups for harder instances: there is only a single instance
whose sequential runtime is over two seconds where we obtain a speedup of less than ten. We also see
a significant number of superlinear speedups. This is because there are relatively long proofs of
optimality for most of the harder instances, but strong solutions are usually found quickly (and
where they are not, the additional diversity helps). The results are similarly positive on
labelled graphs (centre left), with speedups of greater than ten being the norm for harder
instances, although there is more variety in the results.

For $k{\downarrow}$ on the unlabelled maximum common subgraph instances (top centre), the typical speedup
is between two and ten, and occasionally there are speedups of between ten and one hundred. A few
instances show small slowdowns: remember that when running 32 threads on a 16 core, hyper-threaded
system, each individual thread runs somewhat slower than it would if only a single thread were
running---in other words, these slight slowdowns are due to hardware effects, not changes to the
search tree.

On the larger subgraph isomorphism instances (bottom left), we see a concentration of speedups
of around ten. We also see strongly superlinear speedups in some cases, with eight
instances finishing in under one second which timed out sequentially at one thousand seconds.
Examining these instances closely shows that in each case, they are satisfiable with either $k = 0$
or very low values of $k$, and that the superlinear speedups are due to the parallel search
introducing early diversity, offsetting an incorrect initial value-ordering heuristic choice. We
also see more instances with slight absolute slowdowns: these are instances where the additional
threads contribute no helpful work to the solution, and where the slowdown due to not having 32
times as many resources when using 32 threads is particularly pronounced (memory contention is much
more of a problem for these larger graphs). Although not ideal, in aggregate these results are rare
and are more than offset by the superlinear speedups for other instances.

\begin{figure}[p]
    \centering
    \hspace*{1em}\includegraphics*[scale=0.7]{plots/plain-clique-par-repeat-heatmap.pdf}\hfill
    \includegraphics*[scale=0.7]{plots/plain-clique-par-8to32-heatmap.pdf}\hspace*{1em}

    \vspace*{1em}
    \centering
    \hspace*{1em}\includegraphics*[scale=0.7]{plots/plain-kdown-par-repeat-heatmap.pdf}\hfill
    \includegraphics*[scale=0.7]{plots/plain-kdown-par-8to32-heatmap.pdf}\hspace*{1em}

    \vspace*{1em}
    \centering
    \hspace*{1em}\includegraphics*[scale=0.7]{plots/plain-james-par-repeat-heatmap.pdf}\hfill
    \includegraphics*[scale=0.7]{plots/plain-james-par-8to32-heatmap.pdf}\hspace*{1em}

    \caption{Parallel search is reproducible and scalable, as well as risk-free and beneficial. In the
    left-hand column, we compare each of the three parallel algorithms against a repeated run with
    the same parameters, on the unconnected, unlabelled instances (points on the diagonal line show
    reproducibility of runtimes). In the right-hand column, we compare going from 8 threads to 32
    threads (points below the diagonal line are an improvement). All experiments are on a 16 core,
    hyper-threaded system.}
    \label{figure:mcs-par-repeats}
\end{figure}

What about the splitting algorithm? On unlabelled instances (top right), our speedups are typically
between those obtained from the clique algorithm and those from $k{\downarrow}$, being mainly a
little below ten times. However, in some cases we see absolute slowdowns of up to four times: in
these cases, parallelism does little, and is not enough to offset the costs we paid when switching
from an in-place data structure to a data structure which requires copying. Interestingly, we also
see many more superlinear speedups for this algorithm, which suggests that its value ordering
heuristics are particularly poor early-on in search. (This last point is not particularly
surprising: compared to $k{\downarrow}$, the splitting algorithm does not try to reduce domains at
all at the top of search. The clique encoding, meanwhile, is able to capture even richer information
about variable-value relationships. The splitting algorithm is fast, but comparatively stupid.)

Similar trends occur with the subgraph isomorphism instances (bottom right), and the absolute
slowdowns go as high as a factor of ten---this is on instances with particularly large
target graphs, where copying is most expensive, and where there is a high branching factor at the
root of the search tree. Again, superlinear speedups are common.

On labelled instances (centre right), the parallel performance is less erratic. Although we no
longer see absolute slowdowns (excluding on very easy instances), we also see fewer superlinear
speedups, with most speedups being between one and ten. In many of these instances, parallelism
does very little: restricting splitting to a depth of five in these cases gives work balance
problems, but does help us avoid the substantial overheads that come with deeper splitting. This is
because the search trees for these instances tend to have a very low branching factor---in
constraint programming terms, the labels make each domain contain only a few values.

\Cref{figure:mcs-par-unconnected-cumulative,figure:mcs-par-speedups} show that for all three
algorithms, parallel search is beneficial, and also (reasonably) risk-free: although in a few cases the
constant factor slowdowns due to hardware limitations and having to make changes to data structures
are unpleasant, we do not encounter any exponential slowdowns due to search tree changes. But what
about reproducibility and scalability? Further experiments in \cref{figure:mcs-par-repeats}
establish that both of these properties also hold. In the left-hand column, we plot each algorithm
run against itself, using 32 threads in both cases. For the clique and $k{\downarrow}$ approaches,
every instance is on the central diagonal line, showing an extremely high level of reproducibility.
For the splitting algorithm, there is a little more variability, particularly for easier instances:
inspecting the results suggests that this is not down to different amounts of search work being
performed, but rather whether or not helper threads end up triggering additional copying.

In the right-hand column of \cref{figure:mcs-par-repeats}, we plot the effects of moving from 8
threads (on the same system) to 32 threads. Points below the diagonal line indicate an improvement.
For the clique approach, increasing the number of threads is unequivocally beneficial. For the
$k{\downarrow}$ algorithm, the benefits are slim, and sometimes performance worsens slightly due to
memory contention (not work done) but we do not see any exponential slowdowns. Finally, for the
splitting algorithm, the results are usually strictly better, although in a few cases the overheads
increase further giving a slowdown; we also see an increase in the occurrence of strongly
superlinear speedups.

It is likely that the parallel results for the splitting algorithm can be improved further. In the
labelled case, we are often not obtaining a good work balance. However, deeper splitting is
expensive, particularly with larger graphs. One option to consider would be recomputing parts of the
search tree to avoid the costs of potentially allowing a copy in the future. This would be extremely
unpleasant from an implementation perspective, but may be necessary to obtain more even results.
However, even without this change, the parallel algorithm is clearly preferable to the sequential
one.

\section{Conclusion}

\bibliographystyle{splncs03}
\bibliography{dblp}

\end{document}
